# 准备

## 目标

基于llama.cpp部署模型至移动端（IOS+Android）

## 难点

llama.cpp部署模型至linux或者windows都很简单，部署后都能够提供接口调用。但是移动端部署需要交叉编译。

比如在android中编译为android的可执行文件，编译好的二进制代码应该打包到so动态链接库中，在 Android 中，.so 文件是直接打包进 APK 的，不会参与 Java 的编译过程。

android的app应用应该可以通过JNI调用大模型服务

## 需要学习的东西



## 完成的事


1. android ndk 手动拷贝llamacpp源码目录src下的几个最小依赖文件，交叉编译成android平台so库，并且尝试JNI调用
2. android 下termux环境尝试llamacpp的编译
3. llamacpp 手动编译linux版本的libso动态链接库，读了下源码矩阵计算和计算图实现部分
4. react native 几个核心组件学习了一下
5. llama.rn 了解了下它从JS引擎到java到cpp的流程和架构，一些关键模型加载和初始化的代码，最后按照给的sdk写了个部署3b q8_0模型且性能达到15tps（cpu推理）的demo到android上，并感受量化模型的回答准确度以及是否存在幻觉和重复的问题。


llamacpp加载多模态模型并对比模型效果和性能表现，感受cpu推理模型量化的性能提升，onnx+onnxruntime在hg的optimum库的实现，根据demo模型量化部署pc并观察性能，阅读pocketpal-ai项目源码。


7-18:
pocketpal项目从头构建模仿，从最小依赖开始测试DAO层，store层模型管理下载，加载，初始化功能
看了下llama实践过程中遇到的kv缓存问题

ndk预编译android平台的llamacpp核心依赖，手动引入链接库和核心功能头文件，根据开发的api编写模型初始化，会话功能，编写对应makelist编译文件和JNI调用。完成实际app的demo开发，部署smol模型和qwen。

7-21
vlm多模态模型支持（rn）已完成

探索llama.cpp新老api区别和使用方法，老api示例的demo手动找到未链接到动态库的静态库并手动链接到demo，探索采样器链，topk，temperature等链路，感受生成结果准确性随机性。

配置vulkanSDK，系统老旧，手动编译sdk以来的gcc-10.1和glibc的多个版本，找到兼容的版本。

## 下一步

采样器，最新api的chat

探究为什么卸载到GPU的层数越多性能越差，会不会是示例的android代码不行，试试simple那个示例用GPU会不会快

<!-- vulkan，opencl，GPU加速 -->

<!-- 对比多个模型，不同参数，cpu/gpu的推理速度 -->

多模态，原生llama.cpp

模型下载管理
