# 准备

## 目标

基于llama.cpp部署模型至移动端（IOS+Android）

## 难点

llama.cpp部署模型至linux或者windows都很简单，部署后都能够提供接口调用。但是移动端部署需要交叉编译。

比如在android中编译为android的可执行文件，编译好的二进制代码应该打包到so动态链接库中，在 Android 中，.so 文件是直接打包进 APK 的，不会参与 Java 的编译过程。

android的app应用应该可以通过JNI调用大模型服务

## 需要学习的东西

JNI，llama.cpp实践，llama.cpp-android项目

## 完成的事


1. android ndk 手动拷贝llamacpp源码目录src下的几个最小依赖文件，交叉编译成android平台so库，并且尝试JNI调用
2. android 下termux环境尝试llamacpp的编译
3. llamacpp 手动编译linux版本的libso动态链接库，读了下源码矩阵计算和计算图实现部分
4. react native 几个核心组件学习了一下
5. llama.rn 了解了下它从JS引擎到java到cpp的流程和架构，一些关键模型加载和初始化的代码，最后按照给的sdk写了个部署3b q8_0模型且性能达到15tps（cpu推理）的demo到android上，并感受量化模型的回答准确度以及是否存在幻觉和重复的问题。


llamacpp加载多模态模型并对比模型效果和性能表现，感受cpu推理模型量化的性能提升，onnx+onnxruntime在hg的optimum库的实现，根据demo模型量化部署pc并观察性能，阅读pocketpal-ai项目源码。



## 下一步

vlm多模态模型支持

packetpal-ai项目代码